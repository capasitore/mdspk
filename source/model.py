from typing import Listfrom keras import Modelfrom tensorflow import set_random_seedfrom keras.initializers import npfrom keras.backend.tensorflow_backend import expand_dims, squeezefrom keras.layers import Input, Lambda, LSTM, CuDNNLSTM, Bidirectional, Dense, ReLU, TimeDistributed, \                         BatchNormalization, Dropout, ZeroPadding2D, Conv2D, Reshape, Addfrom keras.regularizers import l2def deepspeech(is_gpu: bool, input_dim=26, output_dim=36, context=7,               units=2048, dropouts=[0.05, 0, 0], random_state=1) -> Model:    """    Model is adapted from: Deep Speech: Scaling up end-to-end speech recognition (https://arxiv.org/abs/1412.5567)    It is worth to know which default parameters are used:    - Conv2D:        strides: (1, 1)        padding: "valid"        dilatation_rate: 1        activation: "linear"        use_bias: True        data_format: "channels last"        kernel_initializer: "glorot_uniform"        bias_initializer: "zeros"    - Dense:        activation: "linear"        use_bias: True        kernel_initializer: "glorot_uniform"        bias_initializer: "zeros"    - LSTM (as for CuDNNLSTM):        use_bias: True,        kernel_initializer: "glorot_uniform"        recurrent_initializer: "orthogonal"        bias_initializer: "zeros"        unit_forget_bias: True        implementation: 1        return_state: False        go_backwards: False        stateful: False        unroll: False    """    np.random.seed(random_state)    set_random_seed(random_state)    input_tensor = Input([None, input_dim], name='X')                           # Define input tensor [time, features]    x = Lambda(expand_dims, arguments=dict(axis=-1))(input_tensor)              # Add 4th dim (channel)    x = ZeroPadding2D(padding=(context, 0))(x)                                  # Fill zeros around time dimension    receptive_field = (2*context + 1, input_dim)                                # Take into account fore/back-ward context    x = Conv2D(filters=units, kernel_size=receptive_field)(x)                   # Convolve signal in time dim    x = Lambda(squeeze, arguments=dict(axis=2))(x)                              # Squeeze into 3rd dim array    x = ReLU(max_value=20)(x)                                                   # Add non-linearity    x = Dropout(rate=dropouts[0])(x)                                            # Use dropout as regularization    x = TimeDistributed(Dense(units))(x)                                        # 2nd and 3rd FC layers do a feature    x = ReLU(max_value=20)(x)                                                   # extraction base on the context    x = Dropout(rate=dropouts[1])(x)    x = TimeDistributed(Dense(units))(x)    x = ReLU(max_value=20)(x)    x = Dropout(rate=dropouts[2])(x)    x = Bidirectional(CuDNNLSTM(units, return_sequences=True) if is_gpu else     # LSTM handle long dependencies                      LSTM(units, return_sequences=True, ),                      merge_mode='sum')(x)    output_tensor = TimeDistributed(Dense(output_dim, activation='softmax'))(x)  # Return at each time step prob along characters    model = Model(input_tensor, output_tensor, name='DeepSpeech')    return modeldef deepspeech_custom(is_gpu: bool, layers: List[dict], input_dim=26, output_dim=36, random_state=1, embeddings=False) -> Model:    np.random.seed(random_state)    set_random_seed(random_state)    constructors = {        'BatchNormalization': lambda params: BatchNormalization(**params),        'Conv2D': lambda params: Conv2D(**params),        'Dense': lambda params: TimeDistributed(Dense(**params)),        'Dropout': lambda params: Dropout(**params),        'LSTM': lambda params: Bidirectional(CuDNNLSTM(**params) if is_gpu else                                             LSTM(activation='tanh', recurrent_activation='sigmoid', **params),                                             merge_mode='sum'),        'ReLU': lambda params: ReLU(**params),        'ZeroPadding2D': lambda params: ZeroPadding2D(**params),        'expand_dims': lambda params: Lambda(expand_dims, arguments=params),        'squeeze': lambda params: Lambda(squeeze, arguments=params),        'squeeze_last_dims': lambda params: Reshape([-1, params['units']])    }    input_tensor = Input([None, input_dim], name='X')    x = input_tensor    for params in layers:        name = params.pop('name')        constructor = constructors[name]        x = constructor(params)(x)    if embeddings:        embed = Dense(units=output_dim, use_bias=False, activation='softmax')        embed_tensor = TimeDistributed(embed, trainable=False)(x)        background = Dense(units=output_dim, activation='softmax',                           kernel_regularizer=l2(embeddings['kernel_regularizer_alpha']),                           bias_regularizer=l2(embeddings['bias_regularizer_alpha']))        background_tensor = TimeDistributed(background)(x)        softmax = Add()        softmax.layer = embed   # Link to easy use in deepspeech.compile_model        softmax.embeddings = [np.loadtxt(embeddings['file'])]        output_tensor = softmax([embed_tensor, background_tensor])    else:        output_tensor = TimeDistributed(Dense(units=output_dim, activation='softmax'))(x)    model = Model(input_tensor, output_tensor, name='DeepSpeech')    return model